{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Y55Rf213nfiW"
      ],
      "authorship_tag": "ABX9TyM8sKrPzSO0TiOUeIiX1SES",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arriyan-aa/AI-Hand-Computer-Vision/blob/main/cv_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><h1> <b>AI-Hand-Computer-Vision </b> </center> </h1>\n",
        "\n",
        "Distinguishing between AI-generated hands vs. real hands. Our project aims to address the issue of AI in art and other media. This is an important problem because the use of AI generated material in art and media is becoming increasingly more common and has brought up discussions of ethics and copyright. This model would help distinguish real human hands versus AI generated ones."
      ],
      "metadata": {
        "id": "8Aq3Lqs9NTsA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ouOocnA5orJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries and Getting Files"
      ],
      "metadata": {
        "id": "S_NW4Giu4yJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7yAlBhioUsW",
        "outputId": "3c6b6549-ebbf-4bf5-d822-975acec88112"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install numpy"
      ],
      "metadata": {
        "id": "YurzApWv5jZ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d94de41c-a061-498c-f240-4dec3944f6f4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "hEwQ-_bs5BUw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Cleaning\n",
        "\n",
        "Making sure that we are not missing any files and that the naming conventions are all uniform"
      ],
      "metadata": {
        "id": "Y55Rf213nfiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to the dataset\n",
        "dataset_path = \"drive/MyDrive/hand-dataset\"\n",
        "\n",
        "# Get the list of all images paths in the dataset\n",
        "image_paths = [os.path.join(dataset_path, \"images\", image_name) for image_name in os.listdir(os.path.join(dataset_path, \"images\"))]\n",
        "# Get the list of all labels paths in the dataset\n",
        "label_paths = [os.path.join(dataset_path, \"labels\", label_name) for label_name in os.listdir(os.path.join(dataset_path, \"labels\"))]\n",
        "\n",
        "print (image_paths)"
      ],
      "metadata": {
        "id": "yxtBSi7vnt09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we already have a list of images' paths and labels' path, stored in variables : image_paths and label_paths\n",
        "print(image_paths)\n",
        "print(label_paths)"
      ],
      "metadata": {
        "id": "F0u4Ekh1oWNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we make a list of only the names of images' files and labels' files.\n",
        "\n",
        "e.g. only 000000000531 without the extension"
      ],
      "metadata": {
        "id": "ZnOcs_MQr3DY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the list of filenames without extensions\n",
        "image_files = {file.split(\"/\")[-1].split(\".\")[0] for file in image_paths}\n",
        "label_files = {file.split(\"/\")[-1].split(\".\")[0] for file in label_paths}"
      ],
      "metadata": {
        "id": "O87Md0GBogV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find extra files in each folder\n",
        "extra_images = image_files - label_files\n",
        "extra_labels = label_files - image_files\n",
        "\n",
        "# Output the results\n",
        "print(f\"Extra images (without corresponding labels): {extra_images}\")\n",
        "print(f\"Extra labels (without corresponding images): {extra_labels}\")"
      ],
      "metadata": {
        "id": "_9cyei4PrYPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for file in extra_images:\n",
        "     os.remove(os.path.join(dataset_path,\"images\", file + '.jpg')) # or '.png' depending on our image format\n",
        "\n",
        "for file in extra_labels:\n",
        "     os.remove(os.path.join(dataset_path,\"labels\", file + '.txt'))\n"
      ],
      "metadata": {
        "id": "GEiKvSQwralR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now removing them from the dataset:"
      ],
      "metadata": {
        "id": "YbSGJgwRr5IJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.path.join(dataset_path,\"images\", file + '.jpg')"
      ],
      "metadata": {
        "id": "cSHM7bJZrdA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check again if it worked:"
      ],
      "metadata": {
        "id": "pWjJzNA6r7Z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the list of all images paths in the dataset\n",
        "image_paths = [os.path.join(dataset_path, \"images\", image_name) for image_name in os.listdir(os.path.join(dataset_path, \"images\"))]\n",
        "\n",
        "# Get the list of all labels paths in the dataset\n",
        "label_paths = [os.path.join(dataset_path, \"labels\", label_name) for label_name in os.listdir(os.path.join(dataset_path, \"labels\"))]\n",
        "\n",
        "# Get the list of filenames without extensions\n",
        "image_files = {file.split(\"/\")[-1].split(\".\")[0] for file in image_paths}\n",
        "label_files = {file.split(\"/\")[-1].split(\".\")[0] for file in label_paths}\n",
        "# Find extra files in each folder\n",
        "extra_images = image_files - label_files\n",
        "extra_labels = label_files - image_files\n",
        "\n",
        "# Output the results\n",
        "print(f\"Extra images (without corresponding labels): {extra_images}\")\n",
        "print(f\"Extra labels (without corresponding images): {extra_labels}\")"
      ],
      "metadata": {
        "id": "bkNWC_AgrekJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "Pip install `ultralytics` and [dependencies](https://github.com/ultralytics/ultralytics/blob/main/requirements.txt) and check software and hardware."
      ],
      "metadata": {
        "id": "2o7mlJ6GBS1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ],
      "metadata": {
        "id": "F2oBfYnTBXCt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67f693a2-3040-4862-b9f1-507bee0f276d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.221 ğŸš€ Python-3.12.12 torch-2.8.0+cu126 CPU (Intel Xeon CPU @ 2.20GHz)\n",
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 39.7/107.7 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOLOv8 may be used directly in the Command Line Interface (CLI) with a `yolo` command for a variety of tasks and modes and accepts additional arguments, i.e. `imgsz=640`. See a full list of available `yolo` [arguments](https://docs.ultralytics.com/usage/cfg/) and other details in the [YOLOv8 Predict Docs](https://docs.ultralytics.com/modes/train/).\n",
        "\n",
        "\n",
        "if CLI the format should be:\n",
        "\n",
        "    yolo TASK MODE ARGS\n",
        "\n",
        "  Where:\n",
        "\n",
        "    TASK (optional) is one of (detect, segment, classify, pose)\n",
        "\n",
        "\n",
        "    MODE (required) is one of (train, val, predict, export, track)\n",
        "\n",
        "\n",
        "    ARGS (optional) are arg=value pairs like imgsz=640 that override defaults.\n",
        "\n",
        "\n",
        "Default ARG values are defined on this page from the cfg/defaults.yaml file.\n"
      ],
      "metadata": {
        "id": "hQtOnh1mBXyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from PIL import Image\n",
        "\n",
        "from ultralytics import YOLO\n",
        "model = YOLO(\"yolo11n-cls.pt\")"
      ],
      "metadata": {
        "id": "G6JrErtFBjjn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42e9e569-2d19-4068-eb60-2162874c144e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-cls.pt to 'yolo11n-cls.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 5.5MB 12.4MB/s 0.4s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n",
        "Train YOLOv8 on Detect, Segment, Classify and Pose datasets. See YOLOv8 Train Docs for more information."
      ],
      "metadata": {
        "id": "lDO41UQXdMkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select YOLOv8 ğŸš€ logger {run: 'auto'}\n",
        "logger = 'Comet' #@param ['Comet', 'TensorBoard']\n",
        "\n",
        "if logger == 'Comet':\n",
        "  %pip install -q comet_ml\n",
        "  import comet_ml; comet_ml.init()\n",
        "elif logger == 'TensorBoard':\n",
        "  %load_ext tensorboard\n",
        "  %tensorboard --logdir ."
      ],
      "metadata": {
        "id": "K60FU6levqIU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46b9dd12-f5d4-4e21-ba85-b70d2c69714b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/766.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m757.8/766.5 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m766.5/766.5 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m comet_ml.init() is deprecated and will be removed soon. Please use comet_ml.login()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In YOLO (You Only Look Once), a YAML file is used for configuration and setup. It specifies parameters such as paths to datasets, model architecture, training hyperparameters, and class names. The YAML file is essential for defining how the YOLO model should be trained and what it should detect, making it an integral part of customizing the YOLO model for specific object detection tasks."
      ],
      "metadata": {
        "id": "2xhU4Zy1vcCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load YOLOv11n-cls, train it on hand dataset for 3 epochs\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolo11n-cls.pt')  # load a pretrained YOLOv11n-cls classification model\n",
        "model.train(data='drive/MyDrive/ComputerVisionData/hand-data', epochs=20,imgsz=640)  # train the model\n",
        "\n",
        "# PARTNER 2 TODO:\n",
        "# Test different epoch values [between 20-30 is a good number]\n",
        "# Look at results, maybe take some rough notes abt what its showing (accuracy, confusions), and tweak dataset to improve\n",
        "\n",
        "# ***\n",
        "# [keep in mind]\n",
        "# An epoch = one full pass over the entire dataset\n",
        "# More epochs = more learning time, but risk of overfitting (the model memorizes the data instead of generalizing)\n",
        "# Since we are finetuning a pre-trained model, 20-30 epochs could be enough to converge without overfitting\n",
        "# ***\n",
        "\n",
        "# ***\n",
        "# [a note abt imgsz]\n",
        "# imgsz = size of image\n",
        "# The larger the number, the more details in the image it considers (ex. small features, textures, shadows)\n",
        "# A larger value will be slower/take up more GPU memory, so try testing lower values if needed (ex. imgsz= 320 or imgsz=416)\n",
        "# ***\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qRV1SKOWxDx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model= YOLO(\"runs/classify/train3/weights/best.pt\")\n",
        "metrics = model.val()  # no arguments needed, dataset and settings remembered\n",
        "print(metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGF4yjc_0cyE",
        "outputId": "7314f60e-b9bb-42a3-dc09-935923255d8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.220 ğŸš€ Python-3.12.12 torch-2.8.0+cu126 CPU (AMD EPYC 7B12)\n",
            "YOLO11n-cls summary (fused): 47 layers, 1,528,586 parameters, 0 gradients, 3.2 GFLOPs\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/drive/MyDrive/hand-dataset/train... found 75 images in 2 classes âœ… \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/drive/MyDrive/hand-dataset/val... found 120 images in 2 classes âœ… \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.4Â±0.1 ms, read: 128.6Â±141.9 MB/s, size: 868.6 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/hand-dataset/val... 120 images, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 120/120 158.5Kit/s 0.0s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% â”â”â”â”â”â”â”â”â”â”â”â” 8/8 0.2it/s 39.3s\n",
            "                   all      0.692          1\n",
            "Speed: 0.0ms preprocess, 113.6ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/classify/val\u001b[0m\n",
            "ultralytics.utils.metrics.ClassifyMetrics object with attributes:\n",
            "\n",
            "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7cb76c09f590>\n",
            "curves: []\n",
            "curves_results: []\n",
            "fitness: 0.8458333313465118\n",
            "keys: ['metrics/accuracy_top1', 'metrics/accuracy_top5']\n",
            "results_dict: {'metrics/accuracy_top1': 0.6916666626930237, 'metrics/accuracy_top5': 1.0, 'fitness': 0.8458333313465118}\n",
            "save_dir: PosixPath('/content/runs/classify/val')\n",
            "speed: {'preprocess': 0.0062350666591252475, 'inference': 113.60776298333046, 'loss': 0.00014874999578751158, 'postprocess': 0.00040699999317439506}\n",
            "task: 'classify'\n",
            "top1: 0.6916666626930237\n",
            "top5: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# running inference with yolo model on single image\n",
        "results=model('drive/MyDrive/hand-dataset/val/ai/15.jpg', save=True)\n",
        "#results\n",
        "for r in results:\n",
        "    print(r.probs)  # print the Probs object containing the detected class probabilities"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJ90dVXt1ABO",
        "outputId": "b9cd1bc0-b369-4a38-ef98-a9c7aa4c7bf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/1 /content/drive/MyDrive/hand-dataset/val/ai/15.jpg: 640x640 ai 0.59, real 0.41, 93.0ms\n",
            "Speed: 21.6ms preprocess, 93.0ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1m/content/runs/classify/predict\u001b[0m\n",
            "ultralytics.engine.results.Probs object with attributes:\n",
            "\n",
            "data: tensor([0.5879, 0.4121])\n",
            "orig_shape: None\n",
            "shape: torch.Size([2])\n",
            "top1: 0\n",
            "top1conf: tensor(0.5879)\n",
            "top5: [0, 1]\n",
            "top5conf: tensor([0.5879, 0.4121])\n"
          ]
        }
      ]
    }
  ]
}